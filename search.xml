<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[DPDK开发环境配置]]></title>
    <url>%2F2017%2F12%2F18%2Fdpdk-env%2F</url>
    <content type="text"><![CDATA[背景最近碰到应用网络性能不高的问题，想从DPDK方面入手解决，故有了这篇blog。第一次接触DPDK，不熟悉，用KVM去尝试部署DPDK的开发环境，适合随时回滚。 安装环境 类型 版本 OS CentOS7 DPDK 17.11 注意，因为开发环境在KVM里，而且DPDK只能使用部分Intel的网卡，所以网卡型号必须选择e1000。建议插入三块网卡，一块用来维持ssh连接，另外两块分别用来收/发数据。 安装步骤更新系统1# yum update 如果有更新内核，则需要重启一次系统，否则后面无法加载内核驱动。 安装开发包1# yum group install "Development Tools" -y 安装头文件1# yum install -y libpcap-devel numactl-devel kernel-devel 下载DPDK并解压123# curl https://fast.dpdk.org/rel/dpdk-17.11.tar.xz &gt; dpdk-17.11.tar.xz# tar -xf dpdk-17.11.tar.xz# cd dpdk-17.11 安装DPDK设置安装路径为”/usr”，并运行安装脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# export DESTDIR="/usr" # ./usertools/dpdk-setup.sh------------------------------------------------------------------------------ RTE_SDK exported as /root/dpdk-17.11---------------------------------------------------------------------------------------------------------------------------------------- Step 1: Select the DPDK environment to build----------------------------------------------------------[1] arm64-armv8a-linuxapp-clang[2] arm64-armv8a-linuxapp-gcc[3] arm64-dpaa2-linuxapp-gcc[4] arm64-dpaa-linuxapp-gcc[5] arm64-thunderx-linuxapp-gcc[6] arm64-xgene1-linuxapp-gcc[7] arm-armv7a-linuxapp-gcc[8] i686-native-linuxapp-gcc[9] i686-native-linuxapp-icc[10] ppc_64-power8-linuxapp-gcc[11] x86_64-native-bsdapp-clang[12] x86_64-native-bsdapp-gcc[13] x86_64-native-linuxapp-clang[14] x86_64-native-linuxapp-gcc[15] x86_64-native-linuxapp-icc[16] x86_x32-native-linuxapp-gcc---------------------------------------------------------- Step 2: Setup linuxapp environment----------------------------------------------------------[17] Insert IGB UIO module[18] Insert VFIO module[19] Insert KNI module[20] Setup hugepage mappings for non-NUMA systems[21] Setup hugepage mappings for NUMA systems[22] Display current Ethernet/Crypto device settings[23] Bind Ethernet/Crypto device to IGB UIO module[24] Bind Ethernet/Crypto device to VFIO module[25] Setup VFIO permissions---------------------------------------------------------- Step 3: Run test application for linuxapp environment----------------------------------------------------------[26] Run test application ($RTE_TARGET/app/test)[27] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd)---------------------------------------------------------- Step 4: Other tools----------------------------------------------------------[28] List hugepage info from /proc/meminfo---------------------------------------------------------- Step 5: Uninstall and system cleanup----------------------------------------------------------[29] Unbind devices from IGB UIO or VFIO driver[30] Remove IGB UIO module[31] Remove VFIO module[32] Remove KNI module[33] Remove hugepage mappings[34] Exit ScriptOption: 选择构建环境因为开发环境是64位的CentOS，所以选择的编译器是x86_64-native-linuxapp-gcc，输入14后按回车就会开始构建。12345678910111213Option: 14building...building...building...Build complete [x86_64-native-linuxapp-gcc]Installation cannot run with T defined and DESTDIR undefined------------------------------------------------------------------------------ RTE_TARGET exported as x86_64-native-linuxapp-gcc------------------------------------------------------------------------------Press enter to continue ... 插入IGB UIO模块1234567Option: 17Unloading any existing DPDK UIO moduleLoading uio moduleLoading DPDK UIO modulePress enter to continue ... 修改HugePages大小12345678910111213Option: 20Removing currently reserved hugepagesUnmounting /mnt/huge and removing directory Input the number of 2048kB hugepages Example: to have 128MB of hugepages available in a 2MB huge page system, enter '64' to reserve 64 * 2MB pagesNumber of pages: 512Reserving hugepagesCreating /mnt/huge and mounting as hugetlbfsPress enter to continue ... 绑定网卡123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354Option: 23Network devices using DPDK-compatible driver============================================&lt;none&gt;Network devices using kernel driver===================================0000:00:03.0 'Virtio network device 1000' if=eth0 drv=virtio-pci unused=igb_uio *Active*0000:00:09.0 '82540EM Gigabit Ethernet Controller 100e' if=ens9 drv=e1000 unused=igb_uio0000:00:0a.0 '82540EM Gigabit Ethernet Controller 100e' if=ens10 drv=e1000 unused=igb_uioOther Network devices=====================&lt;none&gt;Crypto devices using DPDK-compatible driver===========================================&lt;none&gt;Crypto devices using kernel driver==================================&lt;none&gt;Other Crypto devices====================&lt;none&gt;Eventdev devices using DPDK-compatible driver=============================================&lt;none&gt;Eventdev devices using kernel driver====================================&lt;none&gt;Other Eventdev devices======================&lt;none&gt;Mempool devices using DPDK-compatible driver============================================&lt;none&gt;Mempool devices using kernel driver===================================&lt;none&gt;Other Mempool devices=====================&lt;none&gt;Enter PCI address of device to bind to IGB UIO driver: 1234Enter PCI address of device to bind to IGB UIO driver: 0000:00:09.0OKPress enter to continue ... 运行测试程序123456Option: 27 Enter hex bitmask of cores to execute testpmd app on Example: to execute app on cores 0 to 7, enter 0xffbitmask: 123456789101112131415161718192021222324252627282930bitmask: 0xffLaunching appEAL: Detected 8 lcore(s)EAL: Probing VFIO support...EAL: WARNING: cpu flags constant_tsc=yes nonstop_tsc=no -&gt; using unreliable clock cycles !EAL: PCI device 0000:00:03.0 on NUMA socket -1EAL: Invalid NUMA socket, default to 0EAL: probe driver: 1af4:1000 net_virtioEAL: PCI device 0000:00:09.0 on NUMA socket -1EAL: Invalid NUMA socket, default to 0EAL: probe driver: 8086:100e net_e1000_emEAL: PCI device 0000:00:0a.0 on NUMA socket -1EAL: Invalid NUMA socket, default to 0EAL: probe driver: 8086:100e net_e1000_emInteractive-mode selectedUSER1: create a new mbuf pool &lt;mbuf_pool_socket_0&gt;: n=203456, size=2176, socket=0Configuring Port 0 (socket 0)Port 0: 52:54:00:65:FC:04Configuring Port 1 (socket 0)Port 1: 52:54:00:FA:AE:C9Checking link statuses...Donetestpmd&gt; PMD: eth_em_interrupt_action(): Port 0: Link Up - speed 1000 Mbps - full-duplexPort 0: LSC eventPMD: eth_em_interrupt_action(): Port 1: Link Up - speed 1000 Mbps - full-duplexPort 1: LSC eventtestpmd&gt; 1234567891011121314testpmd&gt; startio packet forwarding - ports=2 - cores=1 - streams=2 - NUMA support enabled, MP over anonymous pages disabledLogical Core 1 (socket 0) forwards packets on 2 streams: RX P=0/Q=0 (socket 0) -&gt; TX P=1/Q=0 (socket 0) peer=02:00:00:00:00:01 RX P=1/Q=0 (socket 0) -&gt; TX P=0/Q=0 (socket 0) peer=02:00:00:00:00:00 io packet forwarding - CRC stripping enabled - packets/burst=32 nb forwarding cores=1 - nb forwarding ports=2 RX queues=1 - RX desc=128 - RX free threshold=0 RX threshold registers: pthresh=0 hthresh=0 wthresh=0 TX queues=1 - TX desc=512 - TX free threshold=0 TX threshold registers: pthresh=0 hthresh=0 wthresh=0 TX RS bit threshold=0 - TXQ flags=0x0testpmd&gt; 123456789101112131415161718192021testpmd&gt; stopTelling cores to stop...Waiting for lcores to finish... ---------------------- Forward statistics for port 0 ---------------------- RX-packets: 823959 RX-dropped: 0 RX-total: 823959 TX-packets: 892078 TX-dropped: 0 TX-total: 892078 ---------------------------------------------------------------------------- ---------------------- Forward statistics for port 1 ---------------------- RX-packets: 892106 RX-dropped: 0 RX-total: 892106 TX-packets: 824058 TX-dropped: 0 TX-total: 824058 ---------------------------------------------------------------------------- +++++++++++++++ Accumulated forward statistics for all ports+++++++++++++++ RX-packets: 1716065 RX-dropped: 0 RX-total: 1716065 TX-packets: 1716136 TX-dropped: 0 TX-total: 1716136 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Done.testpmd&gt; 123456789101112131415161718192021222324252627282930313233testpmd&gt; quitShutting down port 0...Stopping ports...DoneClosing ports...DoneShutting down port 1...Stopping ports...DoneClosing ports...DoneBye...Shutting down port 0...Stopping ports...DoneClosing ports...Port 0 is already closedDoneShutting down port 1...Stopping ports...DoneClosing ports...Port 1 is already closedDoneBye...Press enter to continue ...]]></content>
      <categories>
        <category>DPDK</category>
      </categories>
      <tags>
        <tag>DPDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kvm-spice-4k-8k]]></title>
    <url>%2F2016%2F05%2F12%2Fkvm-spice-4k-8k%2F</url>
    <content type="text"><![CDATA[现有问题在Linux下，使用Spice显示KVM虚拟机，默认分辨率最大只能达到2560x1600，如下图： 解决方法修改虚拟机配置文件把虚拟机关机，然后打开在/etc/libvirt/qemu目录的虚拟机的配置文件，不同发行版位置可能略有不同。找到显卡的部分，一般显存是16M，如下：1234&lt;video&gt; &lt;model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='1'/&gt; &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/&gt; &lt;/video&gt; 修改其中的ram、vram和vgamem的值，这里注意vgamem的最大值只能是ram和vram的一半，修改显存为128M（128x1024）后如下：1234&lt;video&gt; &lt;model type='qxl' ram='262144' vram='262144' vgamem='131072' heads='1'/&gt; &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/&gt; &lt;/video&gt; 重载配置文件重新载入配置文件，并且重启虚拟机12# virsh define 配置文件名字# virsh reboot 虚拟机名字 选择高清分辨率重新启动虚拟机后，就可以选择8K分辨率了，如下图： 注意事项因为Spice目前仅仅使用CPU进行解码和渲染，所以在8K分辨率下，显示速度相当慢，更不要说播放视频了。 测试环境测试日期为2016-04-23，测试环境如下： 类型 版本 OS Gentoo 64bit QEMU 2.5.0-r2 Libvirt 1.3.2-r1 Spice 0.13.0]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
        <tag>Spice</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
</search>
